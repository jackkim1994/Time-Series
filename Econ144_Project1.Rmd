---
title: "Economics 144 Project 1: Forecasting Number of Poice Calls in Eugene, Oregon"
author: "Jae Kum (Jackie) Kim, Edward(Jun Jie) Li, Luna Xu, and Austin Steinhart"
date: "1/31/2019"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
documentclass: article
classoption: a4paper
---
```{r, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "README_figs/README-"
)
```
# I. Introduction

</p>The data we are analyzing is the daily number of police calls in Eugene, OR from 09/22/2008 up through approximately 2pm on 12/19/2018. The mean number of calls is 238.2, the maximum is 482 and the minimum is 3.</p>
</p>The number of calls per day had a clear downward trend and what appeared to be the season variation. From our previous knowledge of crime rates, we know that there are the seasonal differences in crimes rates (e.g. homicides spike in the summer) which we would expect would lead to seasonal differences in police calls, making the data ideal to analysis for this project. One caveat is that there is a break in the data in late 2013. Upon further research, we discovered that Eugene Police converted to a new format for categorizing and reporting a crime that is expected to be mandated by the federal government in the near future. Thus we expect this to have some effect on the number of reported police calls. This will be discussed further in Part III.</p>
</p>In order to clean the data, we first sequenced it by date using the “dplyr” package. We then removed the first and last data points (09/22/08 and 12/19/18) as they were both stated to be incomplete according to the owner of the data file. Next, we manipulated the data to produce weekly averages. We decided to use weekly averages rather than daily totals in order to determine a stronger seasonal trend as we would be creating 52 seasonal variable rather than 365. The mean, maximum, and minimum of the weekly averages are 238.3, 360, and 118.1, respectively.</p>

# II. Results
```{r, warning = FALSE, message = FALSE, tidy=TRUE}
#setup
rm(list=ls(all=TRUE))
library(readr)
library(lubridate)
library(dplyr)
library(stats)
library(forecast)
library(car)
library(MASS)
library(forecast)
library(xts)
library(zoo)
require(stats)
library(foreign)
library(nlts)
```

```{r, warning = FALSE, message = FALSE,  tidy=TRUE}
setwd('~/Downloads')
data  <- read.table("Eugene_Police_Calls.csv", header=TRUE, sep = ",")
data = data %>% arrange(date)
date1 = seq(as.Date('2008-09-23'), as.Date('2018-12-18'), by = 'day')
police = zoo(data$calls[(c(-1,-3740))], date1)
police_w = apply.weekly(police, mean)
date2 = seq(as.Date('2008-09-28'), as.Date('2018-12-16'), by = 'week')
police_w = zoo(police_w, date2)
```

## 1. Modeling and Forecasting Trend
### 1a. Time-Series Plot

```{r}
police_ts = ts(police_w, start = 2008 + 38.7143/52, frequency = 52)
t<-seq(2008 + 38.7143/52, 2018 + 49/52, length=length(police_ts))
plot(police_ts, type ="l", main = 'Weekly Police Calls in Eugene, OR',
     xlab = 'Year',
     ylab = 'Number of Calls' )
```

### 1b. Covariance Stationary

</p>The data does not appear to be covariance stationary. It has a structural break during 2013 due to a Federal policy change regarding police reports. In addition, there is clearly a downward trend since 2014.</p>


### 1c. ACF and PACF Plots

```{r}
acf(police_ts, main = 'Autocorrelation of Police Calls')
```

</p>The plot shows a slowly decaying ACF, but it is far from decaying to zero. This confirms that our data is not covariance stationary. </p>

```{r}
pacf(police_ts, main = 'Partial Autocorrelation of Police Calls')
```

</p>PACF shows significant spikes at the first 3 to 4 lags and quickly decays to zero. This may suggest an autoregressive process. </p>

### 1d. Linear and Nonlinear Models 


```{r}
# Linear (Quadratic)
y1=tslm(police_ts~t+I(t^2))
```

#### Linear Fit Plot
```{r, tidy=TRUE}
plot(police_ts,main = 'Linear Model Fit', xlab="Year", ylab="Police Calls")
lines(t,y1$fit,col="red3",lwd=2)
legend(x="topright", legend=c("fitted values", "data"),col=c("red","black"), lty=c(1,1),cex = 0.75,text.font = 2)
```

```{r, tidy=TRUE}
# Nonlinear (exponential)
ds=data.frame(x=t, y=police_ts)
y2=nls(y ~ exp(a + b * t), data=ds, start = list(a = 0, b = 0))
```

#### Nonlinear Fit Plot
```{r, tidy=TRUE}
plot(police_ts,main = 'Nonlinear Model Fit', xlab="Year", ylab="Police Calls")
lines(t,fitted(y2),col="red3",lwd=2)
legend(x="topright", legend=c("fitted values", "data"),col=c("red","black"), lty=c(1,1),cex = 0.75,text.font = 2)
```

### 1e. Residuals 

```{r}
plot(y1$fit, y1$res, main = "Linear Fit Residuals",
     ylab="Residuals", xlab="Fitted values")
     abline(h = 0, lty = 2, col = 'red', lwd = 2)
     grid()
```
</p>High residuals for the linear fit seems to be clustered around fitted values that are greater than 250. This indicates that in the periods of 2012 to 2014 of the data, when police calls demonstrated a sudden increase and decrease,  the model is an unperfect fitt. </p>

```{r}
plot(fitted(y2),residuals(y2), main = 'Nonlinear Fit Residuals',
     ylab="Residuals", xlab="Fitted values",
     col = 'skyblue3')
abline(h = 0, lty = 2, col = 'black', lwd = 2)
grid()
```
</p>For the nonlinear fit, the residuals are mostly positive in the cluster between 2012 and 2014. After 2014, the residuals go into the negative.  It seems that due to the sudden increase in calls between 2012 and 2014 and the sudden drop at the end of 2014, the model is unable to fit well. The nonlinear fit plot showcases a clear pattern in trend.</p>

### 1f. Histogram of Residuals

```{r}
truehist(residuals(y1),
         main = 'Linear Fit Residuals',
         xlab = 'Residuals',
ylab = 'Frequency',
         col = 'red')
```
<p>The histogram of residuals for the linear fit model is mostly normal with a mean at 0. Skewness is also minimal. This satisfies the residuals requirement for linear regression.</p>


```{r}
truehist(residuals(y2),
         main = 'Nonlinear Fit Residuals',
ylab = 'Frequency',
         xlab = 'Residuals',
         col = 'skyblue3')
```

</p>The histogram for nonlinear fit residuals is right skewed and the mean is a little bit less than zero. Its normality is definitely less convincing than that of the linear fit model.</p> 

### 1g. Diagnostic
```{r}
# Linear Fit
summary(y1)
```
</p>Linear model y1 produces an adjusted R^2 of 0.7303, F statistic of 722.54, a p-value that is practically zero. Given these statistics, this model is highly significant. For the most part, the regression line reflects the overall trend of the data.  </p>

```{r}
# Nonlinear Fit
summary(y2)
```
</p>The non-linear model does not provide R^2 or F-statistic as they are only calculated for linear regression models. However, the residuals standard error is 36.82. Residual standard error for the non-linear model is greater than the same error for the linear model, which suggests that the sum errors of the non-linear model is greater. Visually speaking, the non-linear model does look like a worse fit. Low p values for both predictors show high model significance.</p>

### 1h. Model Selection 
```{r}
AIC(y1,y2)
BIC(y1,y2)
```
</p>The models both agree on y1. We will be selecting y1.</p>

### 1i. Forecasting One Year Ahead
```{r, tidy=TRUE}
t_pred<-seq(t[length(t)], t[length(t)]+1, length=52)
tn=data.frame(t=t_pred)
pred = predict(tslm(police_ts~t+I(t^2)),tn,interval="predict")
plot(police_ts, main = 'One Year Trend Forecast',
     xlab = 'Year',
     ylab = 'Number of Calls', 
     ylim=c(0,350),xlim=c(2008,2020))
lines(y1$fit,col="red", lwd = 2)
lines(t_pred,pred[,1],col="blue", lwd = 3)
lines(t_pred,pred[,2],col='forestgreen',lwd = 2, lty = 2)
lines(t_pred,pred[,3],col='forestgreen',lwd = 2, lty = 2)
legend(x="topright", legend=c("prediction interval", "fitted values", "data","forecast"),col=c("forestgreen","red","black","blue"), lty=c(2,1,1,1),
cex = 0.75,  text.font = 2)

```

## 2. Modeling and Forecasting Seasonality
### 2a. Seasonal Model

```{r}
y3<- tslm(police_ts ~ season+0)
summary(y3)
```

### 2b. Plotting Seasonal Factors
```{r, tidy=TRUE}
plot(y3$coef,ylab='Seasonal Factors', xlab="Week of the Year",lwd=2, main="Plot of Seasonal Factors", type="l") 
```
</p>Several of the seasonal factors are indeed significant at either the 0.05 or 0.01 level. The plot demonstrates a few week trend when the number of police calls go from highest to lowest. One interesting note: At around the 30th week of the year which would correspond the the beginning of the spring, there is spike in police calls. The opposite analysis goes for fall when the number of police calls drop. This may be able to be explained by the rising crime rate in the spring and summer due to increasing temperatures while the decrease can be explained by the lowering temperatures in the fall.</p>

### 2c. Trend + Seasonal Model
```{r}
y4 <- tslm(police_ts~poly(trend,2) + season)
plot(y4$fit, y4$res, main = 'Residuals of Trend+Seasonal', ylab="Residuals", xlab="Fitted values")
     abline(h = 0, lty = 2, col = 'red', lwd = 2)
     grid()

```

</p>Again, the residual plot exhibits higher variance at higher fit values. The heteroskedasticity is likely caused by the lack of fitness between year 2012 and 2014. Compared to the residuals vs. fitted plot in part 1, the overall magnitudes of the residuals are smaller here, suggesting a better fit.</p>

### 2d. Summary Statistics
```{r}
summary(y4)
accuracy(y4)
```

</p>This model has a high R^2 of 0.7852. A highly significant F-stat of 33.11 and a p-value that is approximately 0. Most of the seasonal variables are highly significant to the 0.001 level.  The peak of police calls in terms of season is week 29, which is around the beginning of the spring. This is probably due to increased outdoor activities due to increased temperatures leading to more police activity.</p>
</p>Looking at the residuals, it seems residuals mostly cluster at the higher call values of >250. This indicates a similar analysis to part I of the project where our linear and non-linear models have a hard time fitting the high call volumes between 2012 and 2014 and the sudden structural break before 2014.</p> 
<p>MAE is around 19.78, showing that the model has a problem in certain areas. While there may be a possibility of outliers, we already removed possible outliers in Part 1. This means the Break in 2013-2014 is contributing some factors of high MAE.</p>

### 2e. Forecasting One Year Ahead
```{r, tidy = TRUE, warning = FALSE, message = FALSE }
y4 <- tslm(police_ts ~ poly(trend,2) +season)
plot(forecast(y4, h=52), main = 'One Year Trend+Seasonal Forecast',
     xlab = 'Year',
     ylab = 'Number of Calls' )
lines(y4$fit, col ='red')
legend(x="topright", legend=c("fitted values", "data","forecast"),col=c("red","black","blue"), lty=c(1,1,1), cex = 0.75,text.font = 2)
```

# III. Conclusion
<p>Our final model y4 successfully incorporates trend and seasonal factors that underpins our data. For most of the data, we observe a downward trend and a five-week seasonal cycle with spike in Summer as well as a dip in winter. y4 follows the trend adequately for the periods before 2013 and after 2014. Predicting one year ahead, we expect to see a continual drop in police calls while retaining a seasonal cycle. The downward trend can be most likely explained by high-quality police work and better economic conditions. However, if the model continues on its current path, it will hit a level of no police calls before the end of 2021. This certainly will not be true. Due to the structural break, in order to improve our model, we should create two separate models, one before the break and one after the break. Due to the break, our current model have a strong downward trend as discussed above. If we instead created two models we would see a much less dramatic trend for the second half of the data. </p>

<p> A second way to improve our model can be inferred from the ACF and the PACF graphs. The ACF graphed showed a decay to zero while the PACF showed a spike at 1, 2, 3, and 4 lags. This would imply that we could fit AR(4) model to it to improve our prediction <p>

<p> Regarding the structural break, after a brief news search we discovered that the Eugene Police converted to a new format for categorizing and reporting crime. The switch was made from the Uniform Crime Reporting (UCR) format to Oregon National Incident Based Reporting System (NIBRS). The reporting systems use divergent rules that if compared might result in inaccurate conclusions about crime rate changes. Under UCR, the top most serious crime is the one the agency reports (with a couple exceptions), and with NIBRS, all of an incident’s crimes (up to a total of 22) are reported. Thus, due to the differences in the way that crime was reported, we suspect that the data regarding police calls may also be affected.<p>

# IV. References
</p>Data is extracted from Kaggle and can be accessed here: https://www.kaggle.com/warrenhendricks/police-call-data-for-eugene-and-springfield-or</p>
